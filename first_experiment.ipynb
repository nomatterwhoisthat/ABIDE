{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG-rkcXIpPko",
        "outputId": "5d4e9a58-0925-44bc-cc4a-d0b84fe06c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/ABIDE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/ABIDE')"
      ],
      "metadata": {
        "id": "fvugb4BSpWQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ИМПОРТЫ И НАСТРОЙКИ ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import scipy.io as sio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Машинное обучение\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import StratifiedGroupKFold, GridSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (roc_auc_score, precision_score, recall_score,\n",
        "                             f1_score, balanced_accuracy_score,\n",
        "                             average_precision_score, matthews_corrcoef,\n",
        "                             accuracy_score, confusion_matrix)\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import lightgbm as lgb\n",
        "\n",
        "print(\"=== ПАЙПЛАЙН ПЕРВОГО ЭКСПЕРИМЕНТА (С DATA LEAKAGE) ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Ny5K_3pf9q",
        "outputId": "cf6cc51f-8473-4859-d193-df1c376442cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ПАЙПЛАЙН ПЕРВОГО ЭКСПЕРИМЕНТА (С DATA LEAKAGE) ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ МЕТРИК ===\n",
        "def safe_auc(y_true, y_score):\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        return np.nan\n",
        "    return roc_auc_score(y_true, y_score)\n",
        "\n",
        "def safe_ap(y_true, y_score):\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        return np.nan\n",
        "    return average_precision_score(y_true, y_score)\n",
        "\n",
        "def compute_metrics(y_true, y_proba, threshold=0.5):\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_proba = np.asarray(y_proba).astype(float)\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        tn = fp = fn = tp = np.nan\n",
        "\n",
        "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"AUC\": safe_auc(y_true, y_proba),\n",
        "        \"AP\": safe_ap(y_true, y_proba),\n",
        "        \"Acc\": accuracy_score(y_true, y_pred) if len(y_true) else np.nan,\n",
        "        \"F1\": f1_score(y_true, y_pred, zero_division=0) if len(y_true) else np.nan,\n",
        "        \"Rec\": recall_score(y_true, y_pred, zero_division=0) if len(y_true) else np.nan,\n",
        "        \"Prec\": precision_score(y_true, y_pred, zero_division=0) if len(y_true) else np.nan,\n",
        "        \"Spec\": spec,\n",
        "        \"BalAcc\": balanced_accuracy_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "        \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp\n",
        "    }\n",
        "\n",
        "def print_split_metrics(split_name, m):\n",
        "    def fmt(x):\n",
        "        if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "            return \"nan\"\n",
        "        if isinstance(x, (int, np.integer)):\n",
        "            return str(int(x))\n",
        "        return f\"{x:.3f}\"\n",
        "\n",
        "    print(\n",
        "        f\"   [{split_name}] \"\n",
        "        f\"AUC={fmt(m['AUC'])}  AP={fmt(m['AP'])}  \"\n",
        "        f\"Acc={fmt(m['Acc'])}  F1={fmt(m['F1'])}  \"\n",
        "        f\"Rec={fmt(m['Rec'])}  Prec={fmt(m['Prec'])}  \"\n",
        "        f\"Spec={fmt(m['Spec'])}  BalAcc={fmt(m['BalAcc'])}   \"\n",
        "        f\"CM(TN,FP,FN,TP)=({fmt(m['TN'])},{fmt(m['FP'])},{fmt(m['FN'])},{fmt(m['TP'])})\"\n",
        "    )"
      ],
      "metadata": {
        "id": "QokkzobJtQ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ФУНКЦИЯ ЗАГРУЗКИ ДАННЫХ ===\n",
        "def load_and_align_data():\n",
        "    \"\"\"\n",
        "    Загрузка данных с гарантией совпадения размеров\n",
        "    \"\"\"\n",
        "    print(\"1. Загрузка и выравнивание данных...\")\n",
        "\n",
        "    # Пути к данным\n",
        "    subject_ids_path = '/content/drive/MyDrive/ABIDE/phenotypic_image_quality/subject_IDs.txt'\n",
        "    pheno_path = '/content/drive/MyDrive/ABIDE/phenotypic.csv'\n",
        "    aal_path = '/content/drive/MyDrive/ABIDE/AAL/original/'\n",
        "\n",
        "    # Загрузка ID субъектов\n",
        "    if os.path.exists(subject_ids_path):\n",
        "        subject_IDs = np.genfromtxt(subject_ids_path, dtype=str)\n",
        "    else:\n",
        "        print(\"ВНИМАНИЕ: Файл с ID не найден, создаем тестовые данные...\")\n",
        "        subject_IDs = [f\"{i:05d}\" for i in range(50001, 50101)]\n",
        "\n",
        "    # Загрузка фенотипических данных\n",
        "    if os.path.exists(pheno_path):\n",
        "        pheno_df = pd.read_csv(pheno_path)\n",
        "    else:\n",
        "        print(\"ВНИМАНИЕ: Фенотипические данные не найдены, создаем тестовые...\")\n",
        "        pheno_df = pd.DataFrame({\n",
        "            'SUB_ID': [int(sid) for sid in subject_IDs],\n",
        "            'DX_GROUP': np.random.choice([1, 2], size=len(subject_IDs), p=[0.5, 0.5]),\n",
        "            'SITE_ID': np.random.choice(['SITE_01', 'SITE_02', 'SITE_03'], size=len(subject_IDs)),\n",
        "            'AGE_AT_SCAN': np.random.uniform(6, 18, size=len(subject_IDs)),\n",
        "            'SEX': np.random.choice(['M', 'F'], size=len(subject_IDs), p=[0.7, 0.3])\n",
        "        })\n",
        "\n",
        "    # Сбор данных с проверкой\n",
        "    matrices = []\n",
        "    meta_data = []\n",
        "\n",
        "    for sid in subject_IDs:\n",
        "        mat_path = os.path.join(aal_path, f\"{sid}.mat\")\n",
        "\n",
        "        if os.path.exists(mat_path):\n",
        "            try:\n",
        "                # Загрузка матрицы\n",
        "                mat = sio.loadmat(mat_path)\n",
        "\n",
        "                if 'connectivity' not in mat:\n",
        "                    continue\n",
        "\n",
        "                conn = mat['connectivity']\n",
        "\n",
        "                # Проверка формы\n",
        "                if conn.shape[0] != conn.shape[1]:\n",
        "                    continue\n",
        "\n",
        "                # Векторизация\n",
        "                triu_idx = np.triu_indices_from(conn, k=1)\n",
        "                features = conn[triu_idx]\n",
        "\n",
        "                # Проверка на NaN/Inf\n",
        "                if np.any(np.isnan(features)) or np.any(np.isinf(features)):\n",
        "                    continue\n",
        "\n",
        "                matrices.append(features)\n",
        "\n",
        "                # Метаданные\n",
        "                sub_info = pheno_df[pheno_df['SUB_ID'] == int(sid)]\n",
        "                if len(sub_info) > 0:\n",
        "                    meta_data.append({\n",
        "                        'subject_id': sid,\n",
        "                        'site': sub_info['SITE_ID'].values[0],\n",
        "                        'age': sub_info['AGE_AT_SCAN'].values[0],\n",
        "                        'sex': 1 if sub_info['SEX'].values[0] == 'M' else 0,\n",
        "                        'diagnosis': 1 if sub_info['DX_GROUP'].values[0] == 1 else 0\n",
        "                    })\n",
        "                else:\n",
        "                    matrices.pop()\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    # Проверяем совпадение размеров\n",
        "    min_len = min(len(matrices), len(meta_data))\n",
        "    matrices = matrices[:min_len]\n",
        "    meta_data = meta_data[:min_len]\n",
        "\n",
        "    # Создаем массивы\n",
        "    X = np.array(matrices)\n",
        "    y = np.array([m['diagnosis'] for m in meta_data])\n",
        "    sites = np.array([m['site'] for m in meta_data])\n",
        "    ages = np.array([m['age'] for m in meta_data])\n",
        "    sexes = np.array([m['sex'] for m in meta_data])\n",
        "    subject_ids = np.array([m['subject_id'] for m in meta_data])\n",
        "\n",
        "    print(f\"   Загружено: X={X.shape}, y={y.shape}, sites={sites.shape}\")\n",
        "    print(f\"   Классы: ASD={sum(y)} ({sum(y)/len(y)*100:.1f}%), \"\n",
        "          f\"Control={len(y)-sum(y)} ({(1-sum(y)/len(y))*100:.1f}%)\")\n",
        "\n",
        "    return X, y, sites, ages, sexes, subject_ids"
      ],
      "metadata": {
        "id": "Fi0Db_Xatfex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ОТБОР ПРИЗНАКОВ (С DATA LEAKAGE) ===\n",
        "def stable_feature_selection_cv(X, y, sites, n_features=60, n_splits=10, random_state=42):\n",
        "    \"\"\"\n",
        "    Стабильный отбор признаков с учетом сайтов (НЕКОРРЕКТНЫЙ - есть data leakage)\n",
        "    \"\"\"\n",
        "    print(f\"\\n3. СУПЕР-СТАБИЛЬНЫЙ отбор признаков (цель: {n_features})...\")\n",
        "\n",
        "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    # Матрица отбора признаков\n",
        "    feature_selection_matrix = np.zeros((X.shape[1], n_splits))\n",
        "\n",
        "    for fold, (train_idx, _) in enumerate(sgkf.split(X, y, groups=sites)):\n",
        "        X_train = X[train_idx]\n",
        "        y_train = y[train_idx]\n",
        "\n",
        "        # Создаем внутренний CV\n",
        "        inner_sgkf = StratifiedGroupKFold(n_splits=3, shuffle=True,\n",
        "                                         random_state=random_state + fold)\n",
        "\n",
        "        fold_feature_scores = np.zeros(X.shape[1])\n",
        "\n",
        "        for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(\n",
        "            inner_sgkf.split(X_train, y_train, groups=sites[train_idx])):\n",
        "\n",
        "            X_inner_train = X_train[inner_train_idx]\n",
        "            y_inner_train = y_train[inner_train_idx]\n",
        "            X_inner_val = X_train[inner_val_idx]\n",
        "            y_inner_val = y_train[inner_val_idx]\n",
        "\n",
        "            # Сильно регуляризованная логистическая регрессия для отбора\n",
        "            scaler = StandardScaler()\n",
        "            X_inner_scaled = scaler.fit_transform(X_inner_train)\n",
        "            X_val_scaled = scaler.transform(X_inner_val)\n",
        "\n",
        "            # Обучаем 3 разных регуляризованных модели\n",
        "            models = [\n",
        "                LogisticRegression(penalty='l1', C=0.005, solver='saga',\n",
        "                                 max_iter=2000, random_state=random_state + inner_fold),\n",
        "                LogisticRegression(penalty='l2', C=0.01, solver='saga',\n",
        "                                 max_iter=2000, random_state=random_state + inner_fold + 100),\n",
        "                LogisticRegression(penalty='elasticnet', l1_ratio=0.5, C=0.01,\n",
        "                                 solver='saga', max_iter=2000,\n",
        "                                 random_state=random_state + inner_fold + 200)\n",
        "            ]\n",
        "\n",
        "            for model in models:\n",
        "                model.fit(X_inner_scaled, y_inner_train)\n",
        "                if hasattr(model, 'coef_'):\n",
        "                    fold_feature_scores += np.abs(model.coef_[0])\n",
        "\n",
        "                # Добавляем score по валидации\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    y_pred = model.predict_proba(X_val_scaled)[:, 1]\n",
        "                    auc = roc_auc_score(y_inner_val, y_pred)\n",
        "                    fold_feature_scores += (auc * np.abs(model.coef_[0]))\n",
        "\n",
        "        feature_selection_matrix[:, fold] = fold_feature_scores\n",
        "\n",
        "    # Вычисляем стабильность\n",
        "    mean_scores = np.mean(feature_selection_matrix, axis=1)\n",
        "    std_scores = np.std(feature_selection_matrix, axis=1)\n",
        "    stability_scores = mean_scores / (std_scores + 1e-10)\n",
        "\n",
        "    # Выбираем ТОЛЬКО самые стабильные признаки\n",
        "    stability_threshold = np.percentile(stability_scores, 90)\n",
        "    stable_mask = stability_scores >= stability_threshold\n",
        "\n",
        "    print(f\"   Найдено {np.sum(stable_mask)} стабильных признаков\")\n",
        "\n",
        "    if np.sum(stable_mask) > n_features:\n",
        "        # Из стабильных берем самые важные\n",
        "        top_stable_idx = np.argsort(mean_scores[stable_mask])[-n_features:]\n",
        "        selected_indices = np.where(stable_mask)[0][top_stable_idx]\n",
        "    else:\n",
        "        selected_indices = np.where(stable_mask)[0]\n",
        "\n",
        "    # Убираем высококоррелированные признаки (>0.85)\n",
        "    X_selected_temp = X[:, selected_indices]\n",
        "    correlation_matrix = np.abs(np.corrcoef(X_selected_temp, rowvar=False))\n",
        "    high_corr_mask = np.any(np.triu(correlation_matrix, 1) > 0.85, axis=0)\n",
        "    final_indices = selected_indices[~high_corr_mask]\n",
        "\n",
        "    print(f\"   После удаления коррелированных: {len(final_indices)} признаков\")\n",
        "\n",
        "    return final_indices, X[:, final_indices]"
      ],
      "metadata": {
        "id": "vBMF4QkTti3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === СОЗДАНИЕ МОДЕЛЕЙ ===\n",
        "def create_ultra_regularized_logistic_regression(X_train, y_train, random_state=42):\n",
        "    \"\"\"\n",
        "    Сильно регуляризованная логистическая регрессия\n",
        "    \"\"\"\n",
        "    print(\"   Обучение СИЛЬНО регуляризованной Logistic Regression...\")\n",
        "\n",
        "    param_grid = {\n",
        "        'C': [0.0001, 0.001, 0.005, 0.01],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'l1_ratio': [0.1, 0.5, 0.9],\n",
        "        'class_weight': ['balanced'],\n",
        "        'solver': ['saga']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        LogisticRegression(max_iter=3000, random_state=random_state),\n",
        "        param_grid,\n",
        "        cv=3,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Калибруем модель\n",
        "    calibrated_model = CalibratedClassifierCV(\n",
        "        grid_search.best_estimator_,\n",
        "        method='isotonic',\n",
        "        cv=3\n",
        "    )\n",
        "\n",
        "    calibrated_model.fit(X_train, y_train)\n",
        "\n",
        "    return calibrated_model\n",
        "\n",
        "\n",
        "def create_ultra_regularized_lightgbm(X_train, y_train, X_val, y_val, random_state=42):\n",
        "    \"\"\"\n",
        "    LightGBM с экстремальной регуляризацией\n",
        "    \"\"\"\n",
        "    print(\"   Обучение СИЛЬНО регуляризованного LightGBM...\")\n",
        "\n",
        "    scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1]) if sum(y_train) > 0 else 1\n",
        "\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 8,\n",
        "        'max_depth': 3,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.5,\n",
        "        'bagging_fraction': 0.5,\n",
        "        'bagging_freq': 1,\n",
        "        'reg_alpha': 1.0,\n",
        "        'reg_lambda': 2.0,\n",
        "        'min_child_samples': 30,\n",
        "        'min_child_weight': 0.01,\n",
        "        'min_split_gain': 0.01,\n",
        "        'scale_pos_weight': scale_pos_weight,\n",
        "        'random_state': random_state,\n",
        "        'n_jobs': -1,\n",
        "        'verbosity': -1,\n",
        "        'max_bin': 64\n",
        "    }\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        valid_sets=[val_data],\n",
        "        num_boost_round=200,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=20, verbose=False),\n",
        "            lgb.log_evaluation(period=0)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "HsRLqU0atmIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ОЦЕНКА МОДЕЛЕЙ ===\n",
        "def evaluate_logistic_regression(X, y, sites, random_state=42):\n",
        "    \"\"\"\n",
        "    Оценка только логистической регрессии\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"_\"*70)\n",
        "    print(\"LOGISTIC REGRESSION\")\n",
        "    print(\"_\"*70)\n",
        "\n",
        "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "\n",
        "    results = {\n",
        "        'test_auc': [], 'test_accuracy': [], 'test_f1': [], 'test_recall': [],\n",
        "        'test_precision': [], 'test_specificity': [],\n",
        "        'test_balanced_acc': [], 'train_auc': [], 'auc_diff': []\n",
        "    }\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups=sites), 1):\n",
        "        print(f\"\\nФолд {fold}/5:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Создаем validation set\n",
        "        X_train_main, X_val, y_train_main, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.15, stratify=y_train, random_state=random_state\n",
        "        )\n",
        "\n",
        "        # Масштабирование\n",
        "        scaler = RobustScaler(quantile_range=(10, 90))\n",
        "        X_train_scaled = scaler.fit_transform(X_train_main)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        print(f\"   Train: {len(y_train_main)}, Val: {len(y_val)}, Test: {len(y_test)}\")\n",
        "\n",
        "        # Обучаем модель\n",
        "        model = create_ultra_regularized_logistic_regression(X_train_scaled, y_train_main, random_state + fold)\n",
        "\n",
        "        # Предсказания\n",
        "        y_proba_train = model.predict_proba(X_train_scaled)[:, 1]\n",
        "        y_proba_test = model.predict_proba(X_test_scaled)[:, 1]\n",
        "        y_proba_val = model.predict_proba(X_val_scaled)[:, 1]\n",
        "\n",
        "        # Оптимальный порог\n",
        "        thresholds = np.linspace(0.3, 0.7, 50)\n",
        "        best_threshold = 0.5\n",
        "        best_f1 = 0\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            y_val_pred = (y_proba_val >= threshold).astype(int)\n",
        "            f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        print(f\"   Оптимальный порог: {best_threshold:.3f} (Val F1={best_f1:.3f})\")\n",
        "\n",
        "        # Метрики\n",
        "        m_train = compute_metrics(y_train_main, y_proba_train, threshold=best_threshold)\n",
        "        m_val   = compute_metrics(y_val, y_proba_val, threshold=best_threshold)\n",
        "        m_test  = compute_metrics(y_test, y_proba_test, threshold=best_threshold)\n",
        "\n",
        "        print_split_metrics(\"TRAIN\", m_train)\n",
        "        print_split_metrics(\"VAL \", m_val)\n",
        "        print_split_metrics(\"TEST\", m_test)\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        train_auc = roc_auc_score(y_train_main, y_proba_train)\n",
        "        y_pred_test = (y_proba_test >= best_threshold).astype(int)\n",
        "        test_auc = roc_auc_score(y_test, y_proba_test)\n",
        "\n",
        "        results['test_auc'].append(test_auc)\n",
        "        results['test_accuracy'].append(accuracy_score(y_test, y_pred_test))\n",
        "        results['test_f1'].append(f1_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_recall'].append(recall_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_precision'].append(precision_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_specificity'].append(recall_score(y_test, y_pred_test, pos_label=0, zero_division=0))\n",
        "        results['test_balanced_acc'].append(balanced_accuracy_score(y_test, y_pred_test))\n",
        "        results['train_auc'].append(train_auc)\n",
        "        results['auc_diff'].append(train_auc - test_auc)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_lightgbm(X, y, sites, random_state=42):\n",
        "    \"\"\"\n",
        "    Оценка только LightGBM\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"_\"*70)\n",
        "    print(\"LIGHTGBM\")\n",
        "    print(\"_\"*70)\n",
        "\n",
        "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "\n",
        "    results = {\n",
        "        'test_auc': [], 'test_accuracy': [], 'test_f1': [], 'test_recall': [],\n",
        "        'test_precision': [], 'test_specificity': [],\n",
        "        'test_balanced_acc': [], 'train_auc': [], 'auc_diff': []\n",
        "    }\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups=sites), 1):\n",
        "        print(f\"\\nФолд {fold}/5:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Создаем validation set\n",
        "        X_train_main, X_val, y_train_main, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.15, stratify=y_train, random_state=random_state\n",
        "        )\n",
        "\n",
        "        # Масштабирование\n",
        "        scaler = RobustScaler(quantile_range=(10, 90))\n",
        "        X_train_scaled = scaler.fit_transform(X_train_main)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        print(f\"   Train: {len(y_train_main)}, Val: {len(y_val)}, Test: {len(y_test)}\")\n",
        "\n",
        "        # Обучаем модель\n",
        "        model = create_ultra_regularized_lightgbm(X_train_scaled, y_train_main, X_val_scaled, y_val, random_state + fold)\n",
        "\n",
        "        # Предсказания\n",
        "        y_proba_train = model.predict(X_train_scaled)\n",
        "        y_proba_test = model.predict(X_test_scaled)\n",
        "        y_proba_val = model.predict(X_val_scaled)\n",
        "\n",
        "        # Оптимальный порог\n",
        "        thresholds = np.linspace(0.3, 0.7, 50)\n",
        "        best_threshold = 0.5\n",
        "        best_f1 = 0\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            y_val_pred = (y_proba_val >= threshold).astype(int)\n",
        "            f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        print(f\"   Оптимальный порог: {best_threshold:.3f} (Val F1={best_f1:.3f})\")\n",
        "\n",
        "        # Метрики\n",
        "        m_train = compute_metrics(y_train_main, y_proba_train, threshold=best_threshold)\n",
        "        m_val   = compute_metrics(y_val, y_proba_val, threshold=best_threshold)\n",
        "        m_test  = compute_metrics(y_test, y_proba_test, threshold=best_threshold)\n",
        "\n",
        "        print_split_metrics(\"TRAIN\", m_train)\n",
        "        print_split_metrics(\"VAL \", m_val)\n",
        "        print_split_metrics(\"TEST\", m_test)\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        train_auc = roc_auc_score(y_train_main, y_proba_train)\n",
        "        y_pred_test = (y_proba_test >= best_threshold).astype(int)\n",
        "        test_auc = roc_auc_score(y_test, y_proba_test)\n",
        "\n",
        "        results['test_auc'].append(test_auc)\n",
        "        results['test_accuracy'].append(accuracy_score(y_test, y_pred_test))\n",
        "        results['test_f1'].append(f1_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_recall'].append(recall_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_precision'].append(precision_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_specificity'].append(recall_score(y_test, y_pred_test, pos_label=0, zero_division=0))\n",
        "        results['test_balanced_acc'].append(balanced_accuracy_score(y_test, y_pred_test))\n",
        "        results['train_auc'].append(train_auc)\n",
        "        results['auc_diff'].append(train_auc - test_auc)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_ensemble(X, y, sites, random_state=42):\n",
        "    \"\"\"\n",
        "    Оценка ансамбля Logistic Regression + LightGBM\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"_\"*70)\n",
        "    print(\"LOGISTIC REGRESSION + LIGHTGBM (АНСАМБЛЬ)\")\n",
        "    print(\"_\"*70)\n",
        "\n",
        "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "\n",
        "    results = {\n",
        "        'test_auc': [], 'test_accuracy': [], 'test_f1': [], 'test_recall': [],\n",
        "        'test_precision': [], 'test_specificity': [],\n",
        "        'test_balanced_acc': [], 'train_auc': [], 'auc_diff': []\n",
        "    }\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups=sites), 1):\n",
        "        print(f\"\\nФолд {fold}/5:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Создаем validation set\n",
        "        X_train_main, X_val, y_train_main, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.15, stratify=y_train, random_state=random_state\n",
        "        )\n",
        "\n",
        "        # Масштабирование\n",
        "        scaler = RobustScaler(quantile_range=(10, 90))\n",
        "        X_train_scaled = scaler.fit_transform(X_train_main)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        print(f\"   Train: {len(y_train_main)}, Val: {len(y_val)}, Test: {len(y_test)}\")\n",
        "\n",
        "        # Обучаем обе модели\n",
        "        lr_model = create_ultra_regularized_logistic_regression(X_train_scaled, y_train_main, random_state + fold)\n",
        "        lgb_model = create_ultra_regularized_lightgbm(X_train_scaled, y_train_main, X_val_scaled, y_val, random_state + fold + 100)\n",
        "\n",
        "        # Предсказания\n",
        "        lr_proba_val = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
        "        lgb_proba_val = lgb_model.predict(X_val_scaled)\n",
        "        ensemble_proba_val = (lr_proba_val + lgb_proba_val) / 2\n",
        "\n",
        "        # Оптимальный порог\n",
        "        thresholds = np.linspace(0.3, 0.7, 50)\n",
        "        best_threshold = 0.5\n",
        "        best_f1 = 0\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            y_val_pred = (ensemble_proba_val >= threshold).astype(int)\n",
        "            f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "\n",
        "        # Предсказания на train и test\n",
        "        lr_proba_train = lr_model.predict_proba(X_train_scaled)[:, 1]\n",
        "        lgb_proba_train = lgb_model.predict(X_train_scaled)\n",
        "        ensemble_proba_train = (lr_proba_train + lgb_proba_train) / 2\n",
        "\n",
        "        lr_proba_test = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "        lgb_proba_test = lgb_model.predict(X_test_scaled)\n",
        "        ensemble_proba_test = (lr_proba_test + lgb_proba_test) / 2\n",
        "\n",
        "        print(f\"   Оптимальный порог: {best_threshold:.3f} (Val F1={best_f1:.3f})\")\n",
        "\n",
        "        # Метрики\n",
        "        m_train = compute_metrics(y_train_main, ensemble_proba_train, threshold=best_threshold)\n",
        "        m_val   = compute_metrics(y_val, ensemble_proba_val, threshold=best_threshold)\n",
        "        m_test  = compute_metrics(y_test, ensemble_proba_test, threshold=best_threshold)\n",
        "\n",
        "        print_split_metrics(\"TRAIN\", m_train)\n",
        "        print_split_metrics(\"VAL \", m_val)\n",
        "        print_split_metrics(\"TEST\", m_test)\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        train_auc = roc_auc_score(y_train_main, ensemble_proba_train)\n",
        "        y_pred_test = (ensemble_proba_test >= best_threshold).astype(int)\n",
        "        test_auc = roc_auc_score(y_test, ensemble_proba_test)\n",
        "\n",
        "        results['test_auc'].append(test_auc)\n",
        "        results['test_accuracy'].append(accuracy_score(y_test, y_pred_test))\n",
        "        results['test_f1'].append(f1_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_recall'].append(recall_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_precision'].append(precision_score(y_test, y_pred_test, zero_division=0))\n",
        "        results['test_specificity'].append(recall_score(y_test, y_pred_test, pos_label=0, zero_division=0))\n",
        "        results['test_balanced_acc'].append(balanced_accuracy_score(y_test, y_pred_test))\n",
        "        results['train_auc'].append(train_auc)\n",
        "        results['auc_diff'].append(train_auc - test_auc)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "vU_1_ks9tsTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ВЫВОД РЕЗУЛЬТАТОВ ===\n",
        "def print_model_results(model_name, results, feature_count, sample_count):\n",
        "    \"\"\"\n",
        "    Вывод результатов для конкретной модели\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"РЕЗУЛЬТАТЫ: {model_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Средние значения метрик\n",
        "    mean_test_auc = np.mean(results['test_auc'])\n",
        "    mean_test_accuracy = np.mean(results['test_accuracy'])\n",
        "    mean_test_f1 = np.mean(results['test_f1'])\n",
        "    mean_test_recall = np.mean(results['test_recall'])\n",
        "    mean_test_precision = np.mean(results['test_precision'])\n",
        "    mean_test_specificity = np.mean(results['test_specificity'])\n",
        "    mean_test_balanced_acc = np.mean(results['test_balanced_acc'])\n",
        "    mean_auc_diff = np.mean(results['auc_diff'])\n",
        "\n",
        "    # Стандартные отклонения\n",
        "    std_test_auc = np.std(results['test_auc'])\n",
        "    std_test_accuracy = np.std(results['test_accuracy'])\n",
        "    std_test_f1 = np.std(results['test_f1'])\n",
        "\n",
        "    print(f\"\\nСРЕДНИЕ МЕТРИКИ (по 5 фолдам):\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Метрика':<25} {'Среднее':<10} {'Стандартное отклонение':<25}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'AUC':<25} {mean_test_auc:<10.3f} {std_test_auc:<25.3f}\")\n",
        "    print(f\"{'Accuracy':<25} {mean_test_accuracy:<10.3f} {std_test_accuracy:<25.3f}\")\n",
        "    print(f\"{'F1-Score':<25} {mean_test_f1:<10.3f} {std_test_f1:<25.3f}\")\n",
        "    print(f\"{'Recall/Sensitivity':<25} {mean_test_recall:<10.3f} {'':<25}\")\n",
        "    print(f\"{'Precision':<25} {mean_test_precision:<10.3f} {'':<25}\")\n",
        "    print(f\"{'Specificity':<25} {mean_test_specificity:<10.3f} {'':<25}\")\n",
        "    print(f\"{'Balanced Accuracy':<25} {mean_test_balanced_acc:<10.3f} {'':<25}\")\n",
        "    print(f\"{'Разница AUC (Train-Test)':<25} {mean_auc_diff:<10.3f} {'':<25}\")\n",
        "\n",
        "    print(f\"\\nСТАТИСТИКА ДАННЫХ:\")\n",
        "    print(f\"   Количество признаков: {feature_count}\")\n",
        "    print(f\"   Количество образцов:  {sample_count}\")\n",
        "    print(f\"   Отношение признаков к образцам: {feature_count/sample_count:.3f}\")\n",
        "\n",
        "    print(f\"\\nАНАЛИЗ ПЕРЕОБУЧЕНИЯ:\")\n",
        "    if abs(mean_auc_diff) < 0.03:\n",
        "        print(\"   Практически нет переобучения (разница AUC < 0.03)\")\n",
        "    elif abs(mean_auc_diff) < 0.05:\n",
        "        print(\"   Минимальное переобучение (разница AUC < 0.05)\")\n",
        "    elif abs(mean_auc_diff) < 0.07:\n",
        "        print(\"   Умеренное переобучение (разница AUC < 0.07)\")\n",
        "    else:\n",
        "        print(\"   Значительное переобучение (разница AUC >= 0.07)\")\n",
        "\n",
        "    print(f\"\\nСТАБИЛЬНОСТЬ МОДЕЛИ:\")\n",
        "    if std_test_auc < 0.03:\n",
        "        print(\"   Отличная стабильность между фолдами (std AUC < 0.03)\")\n",
        "    elif std_test_auc < 0.05:\n",
        "        print(\"   Хорошая стабильность между фолдами (std AUC < 0.05)\")\n",
        "    else:\n",
        "        print(\"   Нестабильные результаты между фолдами (std AUC >= 0.05)\")\n",
        "\n",
        "    # AUC по фолдам\n",
        "    print(f\"\\nAUC ПО ФОЛДАМ:\")\n",
        "    for i, auc in enumerate(results['test_auc'], 1):\n",
        "        print(f\"   Fold {i}: {auc:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "hucGUMzgtzD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ГЛАВНЫЙ ПАЙПЛАЙН ===\n",
        "def run_all_models_pipeline():\n",
        "    \"\"\"\n",
        "    Главный пайплайн для всех моделей (ПЕРВЫЙ ЭКСПЕРИМЕНТ - с data leakage)\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"ПАЙПЛАЙН ПЕРВОГО ЭКСПЕРИМЕНТА (С DATA LEAKAGE)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # 1. Загрузка данных\n",
        "        X, y, sites, ages, sexes, subject_ids = load_and_align_data()\n",
        "\n",
        "        # 2. Предобработка с удалением выбросов\n",
        "        print(\"\\n2. АГРЕССИВНАЯ предобработка данных...\")\n",
        "        from sklearn.ensemble import IsolationForest\n",
        "        iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "        outliers = iso_forest.fit_predict(X)\n",
        "        inliers = outliers == 1\n",
        "\n",
        "        X = X[inliers]\n",
        "        y = y[inliers]\n",
        "        sites = sites[inliers]\n",
        "\n",
        "        print(f\"   Удалено {np.sum(~inliers)} выбросов\")\n",
        "        print(f\"   Осталось: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "        scaler = RobustScaler(quantile_range=(10, 90))\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # 3. ОТБОР ПРИЗНАКОВ (С DATA LEAKAGE!)\n",
        "        feature_indices, X_selected = stable_feature_selection_cv(\n",
        "            X_scaled, y, sites, n_features=60\n",
        "        )\n",
        "\n",
        "        print(f\"\\nФИНАЛЬНАЯ РАЗМЕРНОСТЬ ДАННЫХ:\")\n",
        "        print(f\"   Признаков: {X_selected.shape[1]}\")\n",
        "        print(f\"   Образцов:  {X_selected.shape[0]}\")\n",
        "        print(f\"   Отношение признаков к образцам: {X_selected.shape[1] / X_selected.shape[0]:.3f}\")\n",
        "\n",
        "        # 4. ОЦЕНКА ВСЕХ МОДЕЛЕЙ\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"НАЧАЛО ОЦЕНКИ МОДЕЛЕЙ\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Logistic Regression\n",
        "        lr_results = evaluate_logistic_regression(X_selected, y, sites, random_state=42)\n",
        "        print_model_results(\"LOGISTIC REGRESSION\", lr_results, X_selected.shape[1], X_selected.shape[0])\n",
        "\n",
        "        # LightGBM\n",
        "        lgb_results = evaluate_lightgbm(X_selected, y, sites, random_state=42)\n",
        "        print_model_results(\"LIGHTGBM\", lgb_results, X_selected.shape[1], X_selected.shape[0])\n",
        "\n",
        "        # Ensemble\n",
        "        ensemble_results = evaluate_ensemble(X_selected, y, sites, random_state=42)\n",
        "        print_model_results(\"LOGISTIC REGRESSION + LIGHTGBM (АНСАМБЛЬ)\", ensemble_results, X_selected.shape[1], X_selected.shape[0])\n",
        "\n",
        "        # Сводная информация\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"СВОДНАЯ ИНФОРМАЦИЯ\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nОБЩАЯ СТАТИСТИКА:\")\n",
        "        print(f\"   Количество образцов: {X_selected.shape[0]}\")\n",
        "        print(f\"   Количество признаков: {X_selected.shape[1]}\")\n",
        "        print(f\"   Баланс классов: ASD={sum(y)} ({sum(y)/len(y)*100:.1f}%), Control={len(y)-sum(y)} ({(1-sum(y)/len(y))*100:.1f}%)\")\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'logistic_regression': {\n",
        "                'test_auc': np.mean(lr_results['test_auc']),\n",
        "                'test_accuracy': np.mean(lr_results['test_accuracy']),\n",
        "                'test_f1': np.mean(lr_results['test_f1']),\n",
        "                'test_recall': np.mean(lr_results['test_recall']),\n",
        "                'test_precision': np.mean(lr_results['test_precision']),\n",
        "                'auc_diff': np.mean(lr_results['auc_diff'])\n",
        "            },\n",
        "            'lightgbm': {\n",
        "                'test_auc': np.mean(lgb_results['test_auc']),\n",
        "                'test_accuracy': np.mean(lgb_results['test_accuracy']),\n",
        "                'test_f1': np.mean(lgb_results['test_f1']),\n",
        "                'test_recall': np.mean(lgb_results['test_recall']),\n",
        "                'test_precision': np.mean(lgb_results['test_precision']),\n",
        "                'auc_diff': np.mean(lgb_results['auc_diff'])\n",
        "            },\n",
        "            'ensemble': {\n",
        "                'test_auc': np.mean(ensemble_results['test_auc']),\n",
        "                'test_accuracy': np.mean(ensemble_results['test_accuracy']),\n",
        "                'test_f1': np.mean(ensemble_results['test_f1']),\n",
        "                'test_recall': np.mean(ensemble_results['test_recall']),\n",
        "                'test_precision': np.mean(ensemble_results['test_precision']),\n",
        "                'auc_diff': np.mean(ensemble_results['auc_diff'])\n",
        "            },\n",
        "            'feature_count': X_selected.shape[1],\n",
        "            'sample_count': X_selected.shape[0]\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nОШИБКА: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {'success': False, 'error': str(e)}"
      ],
      "metadata": {
        "id": "FRfPdvf6uD1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ЗАПУСК ПАЙПЛАЙНА ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ЗАПУСК ПЕРВОГО ЭКСПЕРИМЕНТА (С DATA LEAKAGE)...\")\n",
        "    print(\"ВНИМАНИЕ: Этот пайплайн содержит методологические ошибки!\\n\")\n",
        "\n",
        "    result = run_all_models_pipeline()\n",
        "\n",
        "    if result['success']:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ПАЙПЛАЙН УСПЕШНО ЗАВЕРШЕН!\")\n",
        "        print(\"ВНИМАНИЕ: Результаты завышены из-за data leakage\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nИТОГОВЫЕ РЕЗУЛЬТАТЫ (ЗАВЫШЕННЫЕ):\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Модель':<35} {'AUC':<8} {'Accuracy':<10} {'F1':<8} {'Recall':<8} {'Precision':<10}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Logistic Regression':<35} {result['logistic_regression']['test_auc']:<8.3f} \"\n",
        "              f\"{result['logistic_regression']['test_accuracy']:<10.3f} \"\n",
        "              f\"{result['logistic_regression']['test_f1']:<8.3f} \"\n",
        "              f\"{result['logistic_regression']['test_recall']:<8.3f} \"\n",
        "              f\"{result['logistic_regression']['test_precision']:<10.3f}\")\n",
        "\n",
        "        print(f\"{'LightGBM':<35} {result['lightgbm']['test_auc']:<8.3f} \"\n",
        "              f\"{result['lightgbm']['test_accuracy']:<10.3f} \"\n",
        "              f\"{result['lightgbm']['test_f1']:<8.3f} \"\n",
        "              f\"{result['lightgbm']['test_recall']:<8.3f} \"\n",
        "              f\"{result['lightgbm']['test_precision']:<10.3f}\")\n",
        "\n",
        "        print(f\"{'Logistic Regression + LightGBM':<35} {result['ensemble']['test_auc']:<8.3f} \"\n",
        "              f\"{result['ensemble']['test_accuracy']:<10.3f} \"\n",
        "              f\"{result['ensemble']['test_f1']:<8.3f} \"\n",
        "              f\"{result['ensemble']['test_recall']:<8.3f} \"\n",
        "              f\"{result['ensemble']['test_precision']:<10.3f}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        print(f\"\\n ВАЖНО: Эти результаты содержат data leakage!\")\n",
        "        print(\"Отбор признаков выполнялся на всех данных, включая тестовые.\")\n",
        "        print(\"Для корректных результатов используйте второй пайплайн.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ПАЙПЛАЙН ЗАВЕРШИЛСЯ С ОШИБКОЙ\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Ошибка: {result.get('error', 'Неизвестная ошибка')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ligjwLSmuduE",
        "outputId": "cafec0ef-8dc6-4346-ec3b-840ad86b88f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ЗАПУСК ПЕРВОГО ЭКСПЕРИМЕНТА (С DATA LEAKAGE)...\n",
            "ВНИМАНИЕ: Этот пайплайн содержит методологические ошибки!\n",
            "\n",
            "================================================================================\n",
            "ПАЙПЛАЙН ПЕРВОГО ЭКСПЕРИМЕНТА (С DATA LEAKAGE)\n",
            "================================================================================\n",
            "1. Загрузка и выравнивание данных...\n",
            "   Загружено: X=(871, 6670), y=(871,), sites=(871,)\n",
            "   Классы: ASD=403 (46.3%), Control=468 (53.7%)\n",
            "\n",
            "2. АГРЕССИВНАЯ предобработка данных...\n",
            "   Удалено 44 выбросов\n",
            "   Осталось: X=(827, 6670), y=(827,)\n",
            "\n",
            "3. СУПЕР-СТАБИЛЬНЫЙ отбор признаков (цель: 60)...\n",
            "   Найдено 667 стабильных признаков\n",
            "   После удаления коррелированных: 59 признаков\n",
            "\n",
            "ФИНАЛЬНАЯ РАЗМЕРНОСТЬ ДАННЫХ:\n",
            "   Признаков: 59\n",
            "   Образцов:  827\n",
            "   Отношение признаков к образцам: 0.071\n",
            "\n",
            "================================================================================\n",
            "НАЧАЛО ОЦЕНКИ МОДЕЛЕЙ\n",
            "================================================================================\n",
            "\n",
            "______________________________________________________________________\n",
            "LOGISTIC REGRESSION\n",
            "______________________________________________________________________\n",
            "\n",
            "Фолд 1/5:\n",
            "----------------------------------------\n",
            "   Train: 668, Val: 119, Test: 40\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Оптимальный порог: 0.463 (Val F1=0.729)\n",
            "   [TRAIN] AUC=0.826  AP=0.805  Acc=0.749  F1=0.742  Rec=0.777  Prec=0.709  Spec=0.723  BalAcc=0.750   CM(TN,FP,FN,TP)=(259,99,69,241)\n",
            "   [VAL ] AUC=0.775  AP=0.775  Acc=0.731  F1=0.729  Rec=0.782  Prec=0.683  Spec=0.688  BalAcc=0.735   CM(TN,FP,FN,TP)=(44,20,12,43)\n",
            "   [TEST] AUC=0.883  AP=0.886  Acc=0.825  F1=0.851  Rec=0.909  Prec=0.800  Spec=0.722  BalAcc=0.816   CM(TN,FP,FN,TP)=(13,5,2,20)\n",
            "\n",
            "Фолд 2/5:\n",
            "----------------------------------------\n",
            "   Train: 580, Val: 103, Test: 144\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Оптимальный порог: 0.406 (Val F1=0.766)\n",
            "   [TRAIN] AUC=0.825  AP=0.811  Acc=0.722  F1=0.723  Rec=0.792  Prec=0.665  Spec=0.663  BalAcc=0.728   CM(TN,FP,FN,TP)=(209,106,55,210)\n",
            "   [VAL ] AUC=0.820  AP=0.763  Acc=0.786  F1=0.766  Rec=0.766  Prec=0.766  Spec=0.804  BalAcc=0.785   CM(TN,FP,FN,TP)=(45,11,11,36)\n",
            "   [TEST] AUC=0.762  AP=0.794  Acc=0.729  F1=0.723  Rec=0.680  Prec=0.773  Spec=0.783  BalAcc=0.731   CM(TN,FP,FN,TP)=(54,15,24,51)\n",
            "\n",
            "Фолд 3/5:\n",
            "----------------------------------------\n",
            "   Train: 532, Val: 94, Test: 201\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Оптимальный порог: 0.431 (Val F1=0.745)\n",
            "   [TRAIN] AUC=0.831  AP=0.827  Acc=0.750  F1=0.756  Rec=0.802  Prec=0.715  Spec=0.702  BalAcc=0.752   CM(TN,FP,FN,TP)=(193,82,51,206)\n",
            "   [VAL ] AUC=0.763  AP=0.741  Acc=0.745  F1=0.745  Rec=0.778  Prec=0.714  Spec=0.714  BalAcc=0.746   CM(TN,FP,FN,TP)=(35,14,10,35)\n",
            "   [TEST] AUC=0.743  AP=0.669  Acc=0.692  F1=0.670  Rec=0.741  Prec=0.612  Spec=0.655  BalAcc=0.698   CM(TN,FP,FN,TP)=(76,40,22,63)\n",
            "\n",
            "Фолд 4/5:\n",
            "----------------------------------------\n",
            "   Train: 433, Val: 77, Test: 317\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Оптимальный порог: 0.341 (Val F1=0.705)\n",
            "   [TRAIN] AUC=0.826  AP=0.805  Acc=0.693  F1=0.726  Rec=0.880  Prec=0.618  Spec=0.532  BalAcc=0.706   CM(TN,FP,FN,TP)=(124,109,24,176)\n",
            "   [VAL ] AUC=0.730  AP=0.703  Acc=0.662  F1=0.705  Rec=0.861  Prec=0.596  Spec=0.488  BalAcc=0.674   CM(TN,FP,FN,TP)=(20,21,5,31)\n",
            "   [TEST] AUC=0.801  AP=0.788  Acc=0.634  F1=0.703  Rec=0.907  Prec=0.573  Spec=0.386  BalAcc=0.646   CM(TN,FP,FN,TP)=(64,102,14,137)\n",
            "\n",
            "Фолд 5/5:\n",
            "----------------------------------------\n",
            "   Train: 596, Val: 106, Test: 125\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Оптимальный порог: 0.349 (Val F1=0.783)\n",
            "   [TRAIN] AUC=0.816  AP=0.810  Acc=0.710  F1=0.733  Rec=0.841  Prec=0.650  Spec=0.591  BalAcc=0.716   CM(TN,FP,FN,TP)=(185,128,45,238)\n",
            "   [VAL ] AUC=0.833  AP=0.829  Acc=0.764  F1=0.783  Rec=0.900  Prec=0.692  Spec=0.643  BalAcc=0.771   CM(TN,FP,FN,TP)=(36,20,5,45)\n",
            "   [TEST] AUC=0.739  AP=0.683  Acc=0.648  F1=0.672  Rec=0.833  Prec=0.562  Spec=0.507  BalAcc=0.670   CM(TN,FP,FN,TP)=(36,35,9,45)\n",
            "\n",
            "================================================================================\n",
            "РЕЗУЛЬТАТЫ: LOGISTIC REGRESSION\n",
            "================================================================================\n",
            "\n",
            "СРЕДНИЕ МЕТРИКИ (по 5 фолдам):\n",
            "----------------------------------------------------------------------\n",
            "Метрика                   Среднее    Стандартное отклонение   \n",
            "----------------------------------------------------------------------\n",
            "AUC                       0.786      0.053                    \n",
            "Accuracy                  0.706      0.068                    \n",
            "F1-Score                  0.724      0.067                    \n",
            "Recall/Sensitivity        0.814                               \n",
            "Precision                 0.664                               \n",
            "Specificity               0.611                               \n",
            "Balanced Accuracy         0.712                               \n",
            "Разница AUC (Train-Test)  0.039                               \n",
            "\n",
            "СТАТИСТИКА ДАННЫХ:\n",
            "   Количество признаков: 59\n",
            "   Количество образцов:  827\n",
            "   Отношение признаков к образцам: 0.071\n",
            "\n",
            "АНАЛИЗ ПЕРЕОБУЧЕНИЯ:\n",
            "   Минимальное переобучение (разница AUC < 0.05)\n",
            "\n",
            "СТАБИЛЬНОСТЬ МОДЕЛИ:\n",
            "   Нестабильные результаты между фолдами (std AUC >= 0.05)\n",
            "\n",
            "AUC ПО ФОЛДАМ:\n",
            "   Fold 1: 0.883\n",
            "   Fold 2: 0.762\n",
            "   Fold 3: 0.743\n",
            "   Fold 4: 0.801\n",
            "   Fold 5: 0.739\n",
            "\n",
            "================================================================================\n",
            "\n",
            "______________________________________________________________________\n",
            "LIGHTGBM\n",
            "______________________________________________________________________\n",
            "\n",
            "Фолд 1/5:\n",
            "----------------------------------------\n",
            "   Train: 668, Val: 119, Test: 40\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.447 (Val F1=0.722)\n",
            "   [TRAIN] AUC=0.905  AP=0.892  Acc=0.735  F1=0.769  Rec=0.952  Prec=0.646  Spec=0.547  BalAcc=0.750   CM(TN,FP,FN,TP)=(196,162,15,295)\n",
            "   [VAL ] AUC=0.719  AP=0.699  Acc=0.664  F1=0.722  Rec=0.945  Prec=0.584  Spec=0.422  BalAcc=0.684   CM(TN,FP,FN,TP)=(27,37,3,52)\n",
            "   [TEST] AUC=0.773  AP=0.848  Acc=0.675  F1=0.745  Rec=0.864  Prec=0.655  Spec=0.444  BalAcc=0.654   CM(TN,FP,FN,TP)=(8,10,3,19)\n",
            "\n",
            "Фолд 2/5:\n",
            "----------------------------------------\n",
            "   Train: 580, Val: 103, Test: 144\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.455 (Val F1=0.700)\n",
            "   [TRAIN] AUC=0.882  AP=0.868  Acc=0.738  F1=0.760  Rec=0.909  Prec=0.653  Spec=0.594  BalAcc=0.752   CM(TN,FP,FN,TP)=(187,128,24,241)\n",
            "   [VAL ] AUC=0.748  AP=0.668  Acc=0.650  F1=0.700  Rec=0.894  Prec=0.575  Spec=0.446  BalAcc=0.670   CM(TN,FP,FN,TP)=(25,31,5,42)\n",
            "   [TEST] AUC=0.732  AP=0.763  Acc=0.688  F1=0.713  Rec=0.747  Prec=0.683  Spec=0.623  BalAcc=0.685   CM(TN,FP,FN,TP)=(43,26,19,56)\n",
            "\n",
            "Фолд 3/5:\n",
            "----------------------------------------\n",
            "   Train: 532, Val: 94, Test: 201\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.480 (Val F1=0.680)\n",
            "   [TRAIN] AUC=0.901  AP=0.904  Acc=0.793  F1=0.801  Rec=0.864  Prec=0.747  Spec=0.727  BalAcc=0.796   CM(TN,FP,FN,TP)=(200,75,35,222)\n",
            "   [VAL ] AUC=0.682  AP=0.658  Acc=0.670  F1=0.680  Rec=0.733  Prec=0.635  Spec=0.612  BalAcc=0.673   CM(TN,FP,FN,TP)=(30,19,12,33)\n",
            "   [TEST] AUC=0.710  AP=0.639  Acc=0.652  F1=0.635  Rec=0.718  Prec=0.570  Spec=0.603  BalAcc=0.661   CM(TN,FP,FN,TP)=(70,46,24,61)\n",
            "\n",
            "Фолд 4/5:\n",
            "----------------------------------------\n",
            "   Train: 433, Val: 77, Test: 317\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.414 (Val F1=0.661)\n",
            "   [TRAIN] AUC=0.910  AP=0.905  Acc=0.552  F1=0.672  Rec=0.995  Prec=0.508  Spec=0.172  BalAcc=0.583   CM(TN,FP,FN,TP)=(40,193,1,199)\n",
            "   [VAL ] AUC=0.699  AP=0.683  Acc=0.519  F1=0.661  Rec=1.000  Prec=0.493  Spec=0.098  BalAcc=0.549   CM(TN,FP,FN,TP)=(4,37,0,36)\n",
            "   [TEST] AUC=0.705  AP=0.699  Acc=0.502  F1=0.654  Rec=0.987  Prec=0.489  Spec=0.060  BalAcc=0.523   CM(TN,FP,FN,TP)=(10,156,2,149)\n",
            "\n",
            "Фолд 5/5:\n",
            "----------------------------------------\n",
            "   Train: 596, Val: 106, Test: 125\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.471 (Val F1=0.730)\n",
            "   [TRAIN] AUC=0.862  AP=0.867  Acc=0.730  F1=0.762  Rec=0.912  Prec=0.655  Spec=0.565  BalAcc=0.739   CM(TN,FP,FN,TP)=(177,136,25,258)\n",
            "   [VAL ] AUC=0.796  AP=0.757  Acc=0.708  F1=0.730  Rec=0.840  Prec=0.646  Spec=0.589  BalAcc=0.715   CM(TN,FP,FN,TP)=(33,23,8,42)\n",
            "   [TEST] AUC=0.641  AP=0.539  Acc=0.584  F1=0.653  Rec=0.907  Prec=0.510  Spec=0.338  BalAcc=0.623   CM(TN,FP,FN,TP)=(24,47,5,49)\n",
            "\n",
            "================================================================================\n",
            "РЕЗУЛЬТАТЫ: LIGHTGBM\n",
            "================================================================================\n",
            "\n",
            "СРЕДНИЕ МЕТРИКИ (по 5 фолдам):\n",
            "----------------------------------------------------------------------\n",
            "Метрика                   Среднее    Стандартное отклонение   \n",
            "----------------------------------------------------------------------\n",
            "AUC                       0.712      0.043                    \n",
            "Accuracy                  0.620      0.069                    \n",
            "F1-Score                  0.680      0.042                    \n",
            "Recall/Sensitivity        0.844                               \n",
            "Precision                 0.581                               \n",
            "Specificity               0.414                               \n",
            "Balanced Accuracy         0.629                               \n",
            "Разница AUC (Train-Test)  0.180                               \n",
            "\n",
            "СТАТИСТИКА ДАННЫХ:\n",
            "   Количество признаков: 59\n",
            "   Количество образцов:  827\n",
            "   Отношение признаков к образцам: 0.071\n",
            "\n",
            "АНАЛИЗ ПЕРЕОБУЧЕНИЯ:\n",
            "   Значительное переобучение (разница AUC >= 0.07)\n",
            "\n",
            "СТАБИЛЬНОСТЬ МОДЕЛИ:\n",
            "   Хорошая стабильность между фолдами (std AUC < 0.05)\n",
            "\n",
            "AUC ПО ФОЛДАМ:\n",
            "   Fold 1: 0.773\n",
            "   Fold 2: 0.732\n",
            "   Fold 3: 0.710\n",
            "   Fold 4: 0.705\n",
            "   Fold 5: 0.641\n",
            "\n",
            "================================================================================\n",
            "\n",
            "______________________________________________________________________\n",
            "LOGISTIC REGRESSION + LIGHTGBM (АНСАМБЛЬ)\n",
            "______________________________________________________________________\n",
            "\n",
            "Фолд 1/5:\n",
            "----------------------------------------\n",
            "   Train: 668, Val: 119, Test: 40\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.463 (Val F1=0.729)\n",
            "   [TRAIN] AUC=0.845  AP=0.832  Acc=0.757  F1=0.752  Rec=0.794  Prec=0.715  Spec=0.726  BalAcc=0.760   CM(TN,FP,FN,TP)=(260,98,64,246)\n",
            "   [VAL ] AUC=0.779  AP=0.784  Acc=0.731  F1=0.729  Rec=0.782  Prec=0.683  Spec=0.688  BalAcc=0.735   CM(TN,FP,FN,TP)=(44,20,12,43)\n",
            "   [TEST] AUC=0.889  AP=0.901  Acc=0.825  F1=0.851  Rec=0.909  Prec=0.800  Spec=0.722  BalAcc=0.816   CM(TN,FP,FN,TP)=(13,5,2,20)\n",
            "\n",
            "Фолд 2/5:\n",
            "----------------------------------------\n",
            "   Train: 580, Val: 103, Test: 144\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.414 (Val F1=0.766)\n",
            "   [TRAIN] AUC=0.846  AP=0.836  Acc=0.714  F1=0.736  Rec=0.872  Prec=0.636  Spec=0.581  BalAcc=0.726   CM(TN,FP,FN,TP)=(183,132,34,231)\n",
            "   [VAL ] AUC=0.817  AP=0.734  Acc=0.757  F1=0.766  Rec=0.872  Prec=0.683  Spec=0.661  BalAcc=0.767   CM(TN,FP,FN,TP)=(37,19,6,41)\n",
            "   [TEST] AUC=0.762  AP=0.798  Acc=0.701  F1=0.726  Rec=0.760  Prec=0.695  Spec=0.638  BalAcc=0.699   CM(TN,FP,FN,TP)=(44,25,18,57)\n",
            "\n",
            "Фолд 3/5:\n",
            "----------------------------------------\n",
            "   Train: 532, Val: 94, Test: 201\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.439 (Val F1=0.740)\n",
            "   [TRAIN] AUC=0.835  AP=0.838  Acc=0.739  F1=0.753  Rec=0.825  Prec=0.693  Spec=0.658  BalAcc=0.742   CM(TN,FP,FN,TP)=(181,94,45,212)\n",
            "   [VAL ] AUC=0.762  AP=0.730  Acc=0.723  F1=0.740  Rec=0.822  Prec=0.673  Spec=0.633  BalAcc=0.727   CM(TN,FP,FN,TP)=(31,18,8,37)\n",
            "   [TEST] AUC=0.744  AP=0.672  Acc=0.657  F1=0.646  Rec=0.741  Prec=0.573  Spec=0.595  BalAcc=0.668   CM(TN,FP,FN,TP)=(69,47,22,63)\n",
            "\n",
            "Фолд 4/5:\n",
            "----------------------------------------\n",
            "   Train: 433, Val: 77, Test: 317\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.480 (Val F1=0.703)\n",
            "   [TRAIN] AUC=0.843  AP=0.831  Acc=0.755  F1=0.731  Rec=0.720  Prec=0.742  Spec=0.785  BalAcc=0.753   CM(TN,FP,FN,TP)=(183,50,56,144)\n",
            "   [VAL ] AUC=0.730  AP=0.712  Acc=0.714  F1=0.703  Rec=0.722  Prec=0.684  Spec=0.707  BalAcc=0.715   CM(TN,FP,FN,TP)=(29,12,10,26)\n",
            "   [TEST] AUC=0.799  AP=0.795  Acc=0.738  F1=0.735  Rec=0.762  Prec=0.710  Spec=0.717  BalAcc=0.739   CM(TN,FP,FN,TP)=(119,47,36,115)\n",
            "\n",
            "Фолд 5/5:\n",
            "----------------------------------------\n",
            "   Train: 596, Val: 106, Test: 125\n",
            "   Обучение СИЛЬНО регуляризованной Logistic Regression...\n",
            "   Обучение СИЛЬНО регуляризованного LightGBM...\n",
            "   Оптимальный порог: 0.406 (Val F1=0.783)\n",
            "   [TRAIN] AUC=0.845  AP=0.841  Acc=0.728  F1=0.751  Rec=0.862  Prec=0.665  Spec=0.607  BalAcc=0.735   CM(TN,FP,FN,TP)=(190,123,39,244)\n",
            "   [VAL ] AUC=0.850  AP=0.848  Acc=0.764  F1=0.783  Rec=0.900  Prec=0.692  Spec=0.643  BalAcc=0.771   CM(TN,FP,FN,TP)=(36,20,5,45)\n",
            "   [TEST] AUC=0.742  AP=0.688  Acc=0.640  F1=0.662  Rec=0.815  Prec=0.557  Spec=0.507  BalAcc=0.661   CM(TN,FP,FN,TP)=(36,35,10,44)\n",
            "\n",
            "================================================================================\n",
            "РЕЗУЛЬТАТЫ: LOGISTIC REGRESSION + LIGHTGBM (АНСАМБЛЬ)\n",
            "================================================================================\n",
            "\n",
            "СРЕДНИЕ МЕТРИКИ (по 5 фолдам):\n",
            "----------------------------------------------------------------------\n",
            "Метрика                   Среднее    Стандартное отклонение   \n",
            "----------------------------------------------------------------------\n",
            "AUC                       0.787      0.055                    \n",
            "Accuracy                  0.712      0.066                    \n",
            "F1-Score                  0.724      0.072                    \n",
            "Recall/Sensitivity        0.797                               \n",
            "Precision                 0.667                               \n",
            "Specificity               0.636                               \n",
            "Balanced Accuracy         0.717                               \n",
            "Разница AUC (Train-Test)  0.056                               \n",
            "\n",
            "СТАТИСТИКА ДАННЫХ:\n",
            "   Количество признаков: 59\n",
            "   Количество образцов:  827\n",
            "   Отношение признаков к образцам: 0.071\n",
            "\n",
            "АНАЛИЗ ПЕРЕОБУЧЕНИЯ:\n",
            "   Умеренное переобучение (разница AUC < 0.07)\n",
            "\n",
            "СТАБИЛЬНОСТЬ МОДЕЛИ:\n",
            "   Нестабильные результаты между фолдами (std AUC >= 0.05)\n",
            "\n",
            "AUC ПО ФОЛДАМ:\n",
            "   Fold 1: 0.889\n",
            "   Fold 2: 0.762\n",
            "   Fold 3: 0.744\n",
            "   Fold 4: 0.799\n",
            "   Fold 5: 0.742\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "СВОДНАЯ ИНФОРМАЦИЯ\n",
            "================================================================================\n",
            "\n",
            "ОБЩАЯ СТАТИСТИКА:\n",
            "   Количество образцов: 827\n",
            "   Количество признаков: 59\n",
            "   Баланс классов: ASD=387 (46.8%), Control=440 (53.2%)\n",
            "\n",
            "================================================================================\n",
            "ПАЙПЛАЙН УСПЕШНО ЗАВЕРШЕН!\n",
            "ВНИМАНИЕ: Результаты завышены из-за data leakage\n",
            "================================================================================\n",
            "\n",
            "ИТОГОВЫЕ РЕЗУЛЬТАТЫ (ЗАВЫШЕННЫЕ):\n",
            "--------------------------------------------------------------------------------\n",
            "Модель                              AUC      Accuracy   F1       Recall   Precision \n",
            "--------------------------------------------------------------------------------\n",
            "Logistic Regression                 0.786    0.706      0.724    0.814    0.664     \n",
            "LightGBM                            0.712    0.620      0.680    0.844    0.581     \n",
            "Logistic Regression + LightGBM      0.787    0.712      0.724    0.797    0.667     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " ВАЖНО: Эти результаты содержат data leakage!\n",
            "Отбор признаков выполнялся на всех данных, включая тестовые.\n",
            "Для корректных результатов используйте второй пайплайн.\n"
          ]
        }
      ]
    }
  ]
}